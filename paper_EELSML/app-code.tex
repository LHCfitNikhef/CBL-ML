
\section{Installation and usage of {\tt EELSfitter}}
\label{sec:installation}

In this appendix we provide some instructions about the installation
and the usage of the {\tt EELSfitter} code developed
in this work.
%
The code is publicly available from its GitHub repository
\begin{center}
\url{https://github.com/LHCfitNikhef/EELSfitter}.
\end{center}

\subsection*{Load\_data.py}
The purpose of the first script, {\tt Load\_data.py}, is to read the spectrum
intensities and create dataframes to be used for training the neural network.
%

It reads out the spectrum intensities, automatically selects the energy loss
at which the peak intensity occurs and translates the dataset such that
the peak intensity is centered at $\Delta E =$0. 
%
Further, for each spectrum it returns the normalized intensity by normalizing
over the total area under the spectrum. 
%
The output is two datasets, {\tt df} and {\tt df\_vacuum} which contain the 
information on the in-sample and in-vacuum recorded spectra respectively. 
%
The user needs to upload the spectral data in .txt format to the 'Data' folder
and make sure that the vacuum and in-sample spectra are added to the right one
of the two datasets. 
%
For each of the spectra, the minimum and maximum value of the recorded energy 
loss need to be set manually in {\tt Eloss\_min} and {\tt Eloss\_max}.

\subsection*{Fitter.py}
This file will be used to run the neural network training on the data that was 
uploaded using the {\tt Load\_data.py} file.
%
It involves a few automatic steps to determine the hyper-parameters $\Delta E_I$
and $\Delta E_{II}$, automatically prepares and cuts the data before it is feeded
to the neural network to start the training. 
%
\subsubsection*{0. Import libraries and spectral data}
The spectra are imported from the {\bf load\_data.py} file.

\subsubsection*{1. Derivatives ($\Delta E_I$)}
In order to determine the value for $\Delta E_I$, a dataframe {\tt df\_dx} is created
and it calculates the derivatives of each of the in-sample recorded spectra, 
stored as {\tt df\_dx['derivative y*']}, where {\tt *} is any of the in-sample recorded spectra.
%
The first crossing of any of the derivatives with zero is determined 
and stored as the value of $\Delta E_I$. 
%
\subsubsection*{2. Pseudo-data ($\Delta E_{II}$)}
In this section, it calculates the mean over all vacuum spectra, {\tt df\_mean}, and the ratio of the 
intensity to the uncertainty in each energy loss, {\tt df\_mean['ratio']}. 
%
The value of $\Delta E_{II}$ is determined as the energy loss at which this ratio
drops below 1. 
%
This value $\Delta E_{II}$ is stored together with the value of $\Delta E_I$
as the hyper-parameters for training. 
%
However, if one wishes to use other values for these parameters, for instance for 
cross-validating the best value for $\Delta E_I$, these can be adjusted manually.


\subsubsection*{3. Window and prepare data}

The next step is to only keep the data points with $\Delta E \le \Delta E_I$  
and to drop the points for higher energy loss.
%
Experimental central values and uncertainties are calculated by means of equal width 
discretization, for which the number of bins has to be set as {\tt nbins}. 
%
The default value is 32, which means that we use 32 training inputs spread equally
over the range [$ \Delta E_{min}, \Delta E_I$]. 
%
Note that we use the logarithm of the intensity as training inputs, because this eases
the optimization of the neural network. We will translate this back to the real intensity
values after training.
%
$N_{pseudo}$ pseudo datapoints are added in the range $[\Delta E_{II}, \Delta E_{max}]$, where $ \Delta E_{max}$
is the maximum energy loss value of the recorded spectra. 
%
The values for $N_{pseudo}$ and $\Delta E_{max}$ should be changed manually by 
setting them in {\tt max\_x} and {\tt N\_pseudo}. 
%
The output is a dataframe {\tt df\_full} containing all training data and pseudo data points, 
corresponding to a total of $N_{input}$ (= $n_{bins}$ + $N_{pd}$) training inputs.

\subsubsection*{4. Initialize the NN model}

We are now at the point to define the neural network architecture and to prepare the
training inputs to feed them to the neural network for training. 
%
The function {\tt make\_model()} allows to define the number of hidden layers and 
nodes per layer. The default is an architecture of 1-10-15-5-1, meaning one-dimensional 
input and output with three hidden layers in between.

\subsubsection*{5. Initialize data for NN training}

In the following section, a few subsequent steps are taken to prepare the data
as inputs for the neural network. 
%
First, we initiate 'placeholders' for the variables
{\tt x}, {\tt y} and {\tt sigma}. Placeholders allow us to create our operations and 
build our computation graph, without needing the data itself. 
%
The dimension of the placeholder is defined by {\tt $[{\bf None}, dim]$} where 'dim'
should be set to the dimension of the corresponding variable. In our case we use a 
one-dimensional input, so dim=1. 
%
These placeholders are used to define {\tt predictions}, which is in fact a placeholder that is used 
later to make predictions on inputs {\tt x}. 
%
Next, the dataset is split into 80\% training and
20\% validation data.
%
Also we define a vector {\tt predict\_x} that is used to make a direct prediction after training
on each of the replicas. It consists of $N_{pred}$ data points in the energy loss range
{\tt [pred\_min, pred\_max]}. This vector can be changed to one's preferences. 
%

\subsubsection*{6. Create MC replicas}
The final step to be taken before we can start training is the creation of the MC replicas.
%
This is done automatically using the experimental intensities {\tt train\_y} and uncertainties
{\tt train\_sigma} for a total of {\tt Nrep} replicas. The output is an ($N_{input}, N_{rep}$) 
vector containing all the MC replicas. 

\subsubsection*{7. NN training}
The final part of this script is where the training is done inside the function {\tt function\_train()}. 
%
The cost function, optimizer and learning rate are defined, together with a 'saver' used to 
save the network parameters after each optimization. 
%
We start a loop over {\tt Nrep} replicas to initiate a training session on each of the individual replicas
in series. 
%
For each iteration, the i'th replica {\tt train\_y} and {\tt test\_y} are defined from the total of Nrep 
MC replicas and used as training and validation labels. 
\\

After initializing the cost and the epoch at zero, we have our operation defined, and we can run it in a session. 
%
We create a session object and initialize the network parameters with a global variables initializer.
%
The total number of training epochs per session is defined in {\tt training\_epochs} and it shows intermediate 
results after each number of epochs defined by {\tt display\_step}. 
%
Running the session object over the optimizer and cost function requires knowledge about the values of x and sigma. 
These are defined inside the {\tt feed\_dict} argument. 
%
After each epoch, the average training cost and validation cost are evaluated and the network parameters 
are updated accordingly.
%
Each display step, the graph outputs the training and validation cost. 
Once the maximum number of epochs 
had been reached, the optimal stopping point is determined from 
taking the absolute minimum of the validation cost
and restoring the corresponding network parameters by means of the 'saver' function.
%
From this network graph, one can directly output the prediction on the values of {\tt train\_x} and
the results are stored in the array {\tt predictions\_values}.
%
It is also possible to make predictions on any input vector of choice by feeding 
the vector {\tt predict\_x} to the 
network, which outputs an array {\tt extrapolation}.
%

The datafiles that are finally stored are the following:

\begin{itemize}

\item {\tt Prediction\_k} contains the energy loss {\tt train\_x}, the MC training data {\tt train\_y}
and the ZLP prediction made on the array train\_x, where k is the k'th replica. 
\item {\tt Cost\_k} contains the training and validation error for the k'th training session, 
stored after each display step. 
The minimum of the the validation array is used to restore the parameters of the 'best' net.
\item {\tt Extrapolation\_k} contains the arrays {\tt predict\_x} and the ZLP predictions made on these values. 
\end{itemize}
These textfiles can be retrieved later to evaluate the modeled zero loss peaks.
%
Futher, we store the optimal network parameters after each training session in the folder 'Models/Best\_models'. 
%
These can be loaded later to make predictions on any desired set of inputs. 


\subsection{Analysis.py}
This script is used to analyse the predictions from the NN that are stored in the textfiles. 
%
\subsubsection*{0. Import libraries and spectral data}
The spectra are imported from the {\bf load\_data.py} file.

\subsubsection*{1. Create dataframe with all individual spectra}
In order to later subtract all the predictions from the original individual spectra, we create a datafile
{\tt original} which contains the intensity counts for each of the original input spectra, windowed 
betwee energy values {\tt E\_min} and {\tt E\_max}, which can be set manually.

\subsubsection*{2. Load result files}
In order to import the files that were stored during the NN training, 
one should give this script the right directions to find the prediction .txt files
by adjusting the lines {\tt path\_to\_data} and {\tt path\_predict}, {\tt path\_cost} and {\tt path\_extrapolate}, 
containing the predictions, cost function data and the extrapolation predictions respectively.
%

\subsubsection*{3. Post-selection criteria}
In order to drop the predictions made on the replicas where the neural network was not able to converge,
we select only the datafiles where the final error function was below a certain threshold. 
%
We use as error function the chi2 per data point and we take as the default threshold a value of 3. 
%
Keeping only the files with final chi2 values lower than 3, we import the predictions on these files.
%
Once these datasets have been selected and stored in an array called {\tt use\_files},
we move on to the evaluation of the ZLP predictions. 
%
\subsubsection*{4. Subtraction}

First, we give two definitions: 

\begin{itemize}
\item {\tt def matching()}: this is the matching procedure to match the predicted ZLP to the spectrum
to be subtracted, as explained in Sect.~\ref{sec:results_sample}. 
%
It automatically selects the
values of $\Delta E_I$ and $\Delta E_{II}$ for the training session and defines the matching procedure
correspondingly.
\item {\tt def bandgap()}: here we define how we fit the exponential function to the onset of the bandgap,
which is equal to Eq.~\ref{eq:I1}. 
\end{itemize}


We loop over the total of N$_{rep}$ replicas and read each corresponding prediction from the 
extrapolation file. 
%
This means that we are reading out the prediction made for the vector {\tt predict\_x}. Note that
this prediction is still the logarithm of the experimental intensity, so we take the exponential
to translate this back.
%
For each replica {\tt k}, we create a datafile containing the original spectra intensities 
({\tt original['x*']} and {\tt original['y*']}, the predicted ZLP for this replica ({\tt prediction y}) 
and the predicted ZLP after matching with each spectrum ({\tt match *}). 
%
Ultimately, for each replica we subtract the matched spectrum from the original spectrum 
to obtain the desired subtracted spectra: {\tt dif * = original * - match *}. 
%
This is done for each of the total of N$_{rep}$ replicas and all these results are stored in one big 
dataframe: {\tt total\_replicas}. 
%
This file is stored in the folder 'Data/Results/Replica\_files' such that we can retrieve it at any time
later in the future. 
%
From this file, we will be able to calculate the
statistical variables such as prediction means and uncertainties. 

\subsubsection*{5. Evaluate subtracted spectra}
%
The last part is done in the final section, where we create a {\tt mean\_rep} file that contains
all the median predictions and the upper and lower bounds of the 68\% confidence intervals for 
the predicted ZLP, matched spectra and the subtracted spectra, for each of the original recorded
spectra originally given as an input. 
%
The final piece of code creates a plot of the results, showing the original spectrum, the matched
ZLP and the ZLP-subtracted spectrum including uncertainty bounds. 






















