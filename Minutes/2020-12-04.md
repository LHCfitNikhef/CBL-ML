*  Thanks to the nice work from Isabel and Jaco now we have an independent code that evaluates the thickness of a sample given its single scattering distribution and its zero-loss peak. Applying this procedure to one of the spectra in Laurien's paper (Sample A in Fig. 5.1) they find a thickness of roughly 16nm, and exactly the same number using HyperSpy. So this is a very nice result!

*  We agreed to evaluate the same thickness for Sample B, which is much thinner (should be a few monolayers). Let's see if in this case we find a result which is also consistent with our expectations (and with the indications from Digital Micrograph).

*  Since the thickness calculation is now in good agreement between our own code and hyperspy, we can now move to evaluate the benchmarking of the real and the imaginary parts of the dielectric function. What we agreed to produce is a plot where, for a given spectrum, we compare our calculation for the real and imaginary parts of the dielectric function (including uncertainties) and then on top of that the corresponding HyperSpy prediction, to verify whether or not the same result is obtained.

*  Concerning the training of the ML model, since at this point it is likely that we will need to repeat the training several times (since we will now start to work on spectral images), we agreed to devote some time to timing and optimisation, trying to optimise the training and reduce the computational overhaeads. One option for this is that Isabel and Jaco run their jobs in the Nikhef cluster, which has more than 1100 cores and thus will allow to run efficiently jobs in parallel. It will take some time to setup the code but in the long term it will make training and testing much smoother with a quick feedback loop.

*  If the benchmarking of our calculation of the dielectric is successful, we'd like to move to the processing of spectral images. The first thing we'd like to do is to produce a "thickness map", whereby each pixel in the spectral image is assigned a color depending on their thickness (a heat map essentially). We need to think a bit about the pipeline (since evaluating the thickness map requires knowing the ZLP and SSD per spectrum, and in turns this requires some NN fitting first) but we have some ideas and we are confident we can pull this together.

*  Once the machinery to produce thickness maps is in place, we can produce related images eg for the position of the x-axis crossing of the dielectric function or the value of the bandgap. So a lot of cool applications within our reach here.

*  We also discussed a bit about how independent should be EELSfitter as compared to HyperSpy. We agreed that we want to keep our code independent and use HyperSpy for benchmarks, but we also agreed that it is a good idea to reuse some of the lower-lever classes for example for data I/O and storage. To illustrate this point: if HyperSpy has already a nice class to read and store .dm3 images, it is a bit silly to reinvent the wheel and write the same class again. So we can reuse those bits of hyperspy which are relevant for EELSfitter (with eventually proper attribution of course). 

*  Concerning the calculation of the thickness image, one possible pipeline could be

    i) Read the spectra image, store the npixel \times npixel matrix of EEL spectra

    ii) Evaluate the width of the ZLP

    iii) Define a "signal" region and a "background" region by some simple criterion, eg, for Delta E < 5\times sigma_ZLP this is background, else signal

    iv) Use this classification to evaluate N_ZLP and SSD

    v) Evaluate thickness

    vi) Use v) to define in an automated way training regions for the EELS model, eg. regions of similar thickness. 

    vii) Train model, subtract ZLP, and go back to iv), then v)

but I am sure there are other options. They key us to automate the procedure as much as possible. Let's think about this a bit and then see what is the best option.

