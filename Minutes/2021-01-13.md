* First of all, in this exercise we should always deal with absolute distributions. The normalisation is something which is only relevant for NN training, and in that case one should use a single normalisation for all the spectra (so a global rescaling)

* Clearly since points in the sample correspond to rather different thicknesses, the overall intensity of the ZLP is expected to change quite heavily, as you find

* The integrated intensity shows that there are three clusters of reasonable similar thickness, as one can also observe from the original TEM image, and indeed for those cases the ZLP looks quite similar

* Again, we don't want to normalise, this is not correct and we miss important physical information by doing so

* Note that points that do not fall neatly in any cluster will be few and carry a small weight when constructing the pseudo-data. We can further reduce their weight by computing errors in the pseudo-data as 68%CL intervals. So this is not a problem per se

* This said, it would be nice to deploy an automated clustering algorithm, I propose to use k-means clustering as follows: we define as distance as (T_n - \mu_k)^2/mu_k^2 using the notation in my ML slides, where T_n is the thickness of a given pixel and \mu_k is the center of k-th cluster. Then the algorithm will construct the clusters that minimise the relative difference in thickness (rather than the absolute one). We can then look at the distribution of thicknesses within a given cluster and assess whether or not the procedure is working