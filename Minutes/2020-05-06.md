# Minutes May 6th

## Methodology 


- Store the state of the network after each iteration, so that the predictions can be made with the 'best' network parameters. 
- Apply look-back-stopping to terminate the training at the best test cost (post-selection).
-  The criteria for post-selection need to be varied a posteriori. So we need to save each replica with all its information (chi2s etc) so that we can apply a variety of post-selection criteria in the resulting sample without having to redo the fit again.
- Good post-selection requirements are those which avoid absolute cut-offs. For example one can remove all replicas whose chi2 is 5-sigma from the mean.
- A ReLU activation function in the output layer can be applied to ensure strict positive output values.
- Parallelize the training with specified input and output parameters to upgrade speed.


## Checks
- Train the full model and train only on the separate beam energies. Compare the results of the predictions: they should be the same. In other words:
M1 => input DeltaE, Texp, Ebeam
M2 => input DeltaE (fixed Texp0, Ebeam0)
and check that M1 and M2 give fully consistent results for Ebeam0 and Texp0.

- We want to check how the model behaves if only the data points that 
satisfy

Intensity > f \times IntensityMAX

are used in the fit, where f< 1. The idea is to only fit data from the 
ZLP to avoid possible contamination from possible signals from the sample.

