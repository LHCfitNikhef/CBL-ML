# Minutes 17th March

## Methodology

- A single normalisation has to be applied to all spectra. Normalising each of them separately is not correct (assuming that they correspond to identical data taking conditions) and would bias the results.
- To end up with spectra that have the same binning in energy loss, the best option is to apply some smoothing or interpolation, eg
https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html
    and then comnpute the value of the spectra for the same points. Given that the spectra is evaluated for many data points, any uncertainties associated to this smoothing or interpolation can be neglected as compared to the intrinsic data uncertainties.
- Model uncertainties can be estimated using bootstapping (see eg the slides of by ML lectures). The idea is to divide your dataset in four subsets (random partition) and then train the model in each of them. The resulting spread of models provides an estimate of the associated uncertainties. This estimate can be refined but it is good starting point.

## Checks

- A good model should lead to chi2/ndat \sim 1 in the training data
- For long enough training, we should see the validation chi2 starting to increase (sign of overlearning)
- You can check your code using closure testing, see https://arxiv.org/abs/1809.04392 and references therein. The idea is that you generate fake data from some underlying model, and then you check that you reproduce this same model. By construction, this is going to work, else there is a bug somewhere in the code.
- At some point one needs to implement a suitable stopping criterion, and assess the dependence of the results upon this choice.
