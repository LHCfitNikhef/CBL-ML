# Minutes May 12 + 15

We are moving over to data on the spectra!

## Methodology 


- FWHM (resolution) of the ZLP is approx 0.03 eV. Train the model by taking a window of 3 * FWHM around the peak and fit data in this region. Extrapolate in the region outside the ZLP and use this to predict the shape of the ZLP, including uncertainty region.
- Substraction = Spectrum - ZLP at each replica
- Only use dE as input and intensity as output; train model on log(I).
- Constraint: as dE goes to infinity, log(I) should go to zero (plus a tolerance).
- Add pseudo data in order to meet this constraint.

## Checks

- Check the dependence of the training range (window width around the peak)
- Where to add pseudo data?
In the region where the ratio of (Intensity Sample) / (Intensity ZLP) is big (say for example > 100), we are in the 'data region'. Here the influence of the ZLP in the sample is negligible and we can add pseudo data. How does the restriction on this ratio influence the predictions?

- When can we trust the results?
In the region where the ratio of (Intensity sample) / (uncertainty ZLP) is big, we can 'trust' the substracted spectrum. The signal there is large compared to the uncertainty. If the bandgap is in this region, that's good. 


## Literature study

- Perform literature study about low-loss features of MoS2. This is the motivation for substraction in the first place. 
- Do the features occur in the region where substraction uncertainties are big, or small? So are the results significant?
- Summarize findings for motivation of paper. Motivation is 2 parts:
1) Methodology: why is this method much better than manual substraction?
2) Materials: why is substraction interesting in the first place? What can we learn from the results?

