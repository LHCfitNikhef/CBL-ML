# Minutes 17th March

## Methodology

- Implement 'look-back stopping': let the model run for a longer time and check afterwards what has been the best stopping time under two conditions: A) Chi2_training < treshold (eg 1.5) and B) Chi2_validation has an absolute minimum. Use this to define the best stopping criteria. 
- Construct a Monte Carlo representation to determine the uncertainties in the model (also used in Tutorial 2 of Juan's online course). Goal is to train the model on the Monte Carlo replicas and find the uncertainties; this represents the variances from the original data. 


## Checks

- A good model should lead to chi2/ndat \sim 1 in the training data. Running the training longer should really let the Chi2 approach 1.
- Check what happens when we change the amount of validation data (now it is 0.2, decrease and increase to eg 0.5).
- Repeat training with random seeds such that N_model >> 1 and find uncertaincies in predictions.

## Lookout

### Few notes for in a later stage: 
- Correlations between data points;
- Fit both a NN and a Gaussian in the script to compare the results of both methods;
- Extrapolation: in this region, if the Chi2 goes to values >>1 then the model is not predicting the data we


- For long enough training, we should see the validation chi2 starting to increase (sign of overlearning)
- You can check your code using closure testing, see https://arxiv.org/abs/1809.04392 and references therein. The idea is that you generate fake data from some underlying model, and then you check that you reproduce this same model. By construction, this is going to work, else there is a bug somewhere in the code.
- At some point one needs to implement a suitable stopping criterion, and assess the dependence of the results upon this choice.
