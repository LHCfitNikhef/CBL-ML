{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewd(x, y):  \n",
    "    \"\"\"Apply Equal Width Discretization (EWD) to the training data to determine variances\"\"\"\n",
    "    \n",
    "    df_train = np.stack((x, y)).T\n",
    "    np.matrix(df_train)\n",
    "    df_train = df_train[np.argsort(df_train[:,0])]\n",
    "    cuts1, cuts2 = pd.cut(df_train[:,0], nbins, retbins=True)\n",
    "    \n",
    "    return df_train, cuts1, cuts2\n",
    "\n",
    "def binned_statistics(x,y):\n",
    "    \"\"\"Find the mean, variance and number of counts within the bins described by ewd\"\"\"\n",
    "    \n",
    "    df_train, cuts1, cuts2 = ewd(x,y)\n",
    "    mean, edges, binnum = scipy.stats.binned_statistic(df_train[:,0], df_train[:,1], statistic='mean', bins=cuts2)\n",
    "    std, edges, binnum = scipy.stats.binned_statistic(df_train[:,0], df_train[:,1], statistic='std', bins=cuts2)\n",
    "    count, edges, binnum = scipy.stats.binned_statistic(df_train[:,0], df_train[:,1], statistic='count', bins=cuts2)\n",
    "    \n",
    "    return mean, std, count\n",
    "\n",
    "def vectorize_variance(x,y):\n",
    "    \"\"\"Apply the binned variances to the original training data\"\"\"\n",
    "    \n",
    "    df_train, cuts1, cuts2 = ewd(x,y)\n",
    "    mean, std, count = binned_statistics(x,y)\n",
    "    variance=[]\n",
    "    m=0\n",
    "    i=0\n",
    "    while i<len(count):\n",
    "        maximum = count[i]\n",
    "\n",
    "        while m < maximum:\n",
    "            variance.append(std[i])\n",
    "            m+=1\n",
    "        else:\n",
    "            m=0\n",
    "            i+=1\n",
    "    return np.array(variance)\n",
    "\n",
    "def vectorize_mean(x,y):\n",
    "    \n",
    "    df_train, cuts1, cuts2 = ewd(x,y)\n",
    "    mean, std, count = binned_statistics(x,y)\n",
    "    means=[]\n",
    "    m=0\n",
    "    i=0\n",
    "    while i<len(count):\n",
    "        maximum = count[i]\n",
    "\n",
    "        while m < maximum:\n",
    "            means.append(mean[i])\n",
    "            m+=1\n",
    "        else:\n",
    "            m=0\n",
    "            i+=1\n",
    "    return np.array(means)\n",
    "\n",
    "def plot_uncertainties(x, y, nbins):\n",
    "    nbins\n",
    "    df_train, cuts1, cuts2 = ewd(x, y)\n",
    "    length = len(df_train[:,0])\n",
    "    mean, std, count = binned_statistics(x,y)\n",
    "    variance_vector = vectorize_variance(x,y)\n",
    "    mean_vector = vectorize_mean(x,y)\n",
    "    \n",
    "    upper_limit = mean_vector + variance_vector\n",
    "    lower_limit = mean_vector - variance_vector\n",
    "\n",
    "    ## Plot the output\n",
    "    plt.plot(np.linspace(-0.12, 0.123, nbins), mean, label='Mean')\n",
    "    plt.fill_between(df_train[:,0], upper_limit, lower_limit, color='lightblue', label='Uncertainty (std)')\n",
    "    plt.legend()\n",
    "    \n",
    "\"\"\"Neural network functions: \"\"\"\n",
    "    \n",
    "def custom_cost(y_true, y_pred):\n",
    "    '''Chi square function'''\n",
    "    return tf.reduce_mean(tf.square((y_true-y_pred)/sigma))\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def bootstrap():\n",
    "    df_train_a, df_train_b = train_test_split(df_train, test_size=0.5)\n",
    "    df_train_1, df_train_2 = train_test_split(df_train_a, test_size=0.5)\n",
    "    df_train_3, df_train_4 = train_test_split(df_train_b, test_size=0.5)\n",
    "    \n",
    "    return df_train_1, df_train_2, df_train_3, df_train_4\n",
    "    \n",
    "    \n",
    "def smooth(x,window_len=10,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "    \n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    \n",
    "    index = int(window_len/2)\n",
    "    return y[(index-1):-(index)]\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
