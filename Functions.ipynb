{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewd():  \n",
    "    \"\"\"Apply Equal Width Discretization (EWD) to the training data to determine variances\"\"\"\n",
    "    \n",
    "    df_train = np.stack((x_train, y_train)).T\n",
    "    np.matrix(df_train)\n",
    "    df_train = df_train[np.argsort(df_train[:,0])]\n",
    "    cuts1, cuts2 = pd.cut(df_train[:,0], nbins, retbins=True)\n",
    "    \n",
    "    return df_train, cuts1, cuts2\n",
    "\n",
    "def binned_statistics():\n",
    "    \"\"\"Find the mean, variance and number of counts within the bins described by ewd\"\"\"\n",
    "    \n",
    "    df_train, cuts1, cuts2 = ewd()\n",
    "    mean, edges, binnum = scipy.stats.binned_statistic(df_train[:,0], df_train[:,1], statistic='mean', bins=cuts2)\n",
    "    std, edges, binnum = scipy.stats.binned_statistic(df_train[:,0], df_train[:,1], statistic='std', bins=cuts2)\n",
    "    count, edges, binnum = scipy.stats.binned_statistic(df_train[:,0], df_train[:,1], statistic='count', bins=cuts2)\n",
    "    \n",
    "    return mean, std, count\n",
    "\n",
    "def vectorize_variance():\n",
    "    \"\"\"Apply the binned variances to the original training data\"\"\"\n",
    "    \n",
    "    df_train, cuts1, cuts2 = ewd()\n",
    "    mean, std, count = binned_statistics()\n",
    "    variance=[]\n",
    "    m=0\n",
    "    i=0\n",
    "    while i<len(count):\n",
    "        maximum = count[i]\n",
    "\n",
    "        while m < maximum:\n",
    "            variance.append(std[i])\n",
    "            m+=1\n",
    "        else:\n",
    "            m=0\n",
    "            i+=1\n",
    "    return np.array(variance)\n",
    "\n",
    "def vectorize_mean():\n",
    "    \n",
    "    df_train, cuts1, cuts2 = ewd()\n",
    "    mean, std, count = binned_statistics()\n",
    "    means=[]\n",
    "    m=0\n",
    "    i=0\n",
    "    while i<len(count):\n",
    "        maximum = count[i]\n",
    "\n",
    "        while m < maximum:\n",
    "            means.append(mean[i])\n",
    "            m+=1\n",
    "        else:\n",
    "            m=0\n",
    "            i+=1\n",
    "    return np.array(means)\n",
    "\n",
    "def plot_uncertainties():\n",
    "    \n",
    "    df_train, cuts1, cuts2 = ewd()\n",
    "    mean, std, count= binned_statistics()\n",
    "    variance_vector = vectorize_variance()\n",
    "    mean_vector = vectorize_mean()\n",
    "    \n",
    "    upper_limit = mean_vector + variance_vector\n",
    "    lower_limit = mean_vector - variance_vector\n",
    "\n",
    "   \n",
    "\n",
    "    ## Plot the output\n",
    "    plt.plot(np.linspace(-0.1, 0.1, 60), mean, label='Mean')\n",
    "    plt.fill_between(df_train[:,0], upper_limit, lower_limit, color='lightblue', label='Uncertainty (std)')\n",
    "    plt.legend()\n",
    "    plt.title('Mixed batch of 4,000 ZLPs')\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"Neural network functions: \"\"\"\n",
    "    \n",
    "def custom_cost(y_true, y_pred):\n",
    "    '''Chi square function'''\n",
    "    return tf.reduce_mean(tf.square((y_true-y_pred)/sigma))\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
