{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "import load_data\n",
    "print('Importing packages...')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.compat.v1 as tf\n",
    "from copy import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('done')\n",
    "\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing datafiles...')\n",
    "%run Load_data.py\n",
    "%run Functions.py\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivatives ($\\Delta$E$_I$)\n",
    "In order to determine the values for $\\Delta$E$_I$ and $\\Delta$E$_{II}$, we calculate the derivatives of the intensity of each spectrum with respect to the change energy loss. We use the function 'smooth(y, window_len)' to smooth the functions and reveal underlying trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl1 = 50\n",
    "wl2 = 100\n",
    "\n",
    "df_dx = pd.DataFrame()\n",
    "\n",
    "all_files = [load_data.file14, load_data.file15, load_data.file16, load_data.file17, load_data.file19, load_data.file20, load_data.file21, load_data.file22, load_data.file23]\n",
    "\n",
    "for i,j in enumerate([14,15,16,17,19,20,21,22,23]):\n",
    "    df_dx['x%(j)s' % {\"j\": j}]  =  all_files[i]['x_shifted']\n",
    "    df_dx['y%(j)s' % {\"j\": j}]  =  functions.smooth(all_files[i]['y_norm'], wl1)\n",
    "    df_dx['derivative y%(j)s' %{\"j\": j}] = np.divide(df_dx['y%(j)s'% {\"j\": j}].diff(), \\\n",
    "                                                     df_dx['x%(j)s'% {\"j\": j}].diff())\n",
    "    df_dx['smooth derivative y%(j)s' %{\"j\": j}] = functions.smooth(df_dx['derivative y%(j)s' %{\"j\": j}], wl2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the first crossing with zero of the in-sample derivatives $\\Delta E_{I,min}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for i in ([14,15,16,19,20,21]):\n",
    "    crossing = df_dx[(df_dx['derivative y%(i)s' %{\"i\": i}] > 0) & (df_dx['x%(i)s'% {\"i\": i}] > 1)]['x%(i)s'% {\"i\": i}].min()\n",
    "    li.append(crossing)\n",
    "\n",
    "dE1 = min(li)\n",
    "dE1_min = np.round(dE1, 3)\n",
    "print(\"The value of dE1 is\", dE1_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create plot of the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 3,1\n",
    "gs = matplotlib.gridspec.GridSpec(nrows,ncols)\n",
    "plt.figure(figsize=(ncols*7,nrows*4.5))\n",
    "\n",
    "cm_subsection = np.linspace(0,1,24)\n",
    "colors = [cm.viridis(x) for x in cm_subsection]\n",
    "\n",
    "hfont = rc('font',**{'family':'sans-serif','sans-serif':['Sans Serif']})\n",
    "\n",
    "for i in range(2):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.set_xlim([0,9])\n",
    "    ax.tick_params(which='major',direction='in',length=7)\n",
    "    ax.tick_params(which='minor',length=8)\n",
    "    plt.axhline(y=0, color='black', linewidth=1, alpha=.8)\n",
    "    plt.axvline(x=0, color='darkgray', linestyle='--', linewidth = 1)\n",
    "    #plt.axvline(x=dE1, color='darkgray', linestyle='--', linewidth = 1, label='$\\Delta$E1' %{'s': dE1})\n",
    "\n",
    "    for j in ([17,22,23]):\n",
    "        if i == 0:\n",
    "            p2 = ax.plot(df_dx['x%(i)s'% {\"i\": j}],df_dx['derivative y%(i)s' %{\"i\": j}], color=colors[j], label='%(i)s' %{\"i\": j})\n",
    "\n",
    "    for j in ([14,15,16,19,20,21]):\n",
    "        k = j-3\n",
    "\n",
    "        if i == 0:\n",
    "            p1 = ax.plot(df_dx['x%(i)s'% {\"i\": j}],df_dx['derivative y%(i)s' %{\"i\": j}], color=colors[-k], label='%(i)s' %{\"i\": j})\n",
    "            ax.set_ylim([-.002, .001])\n",
    "            ax.set_xlim([0, 6])\n",
    "            ax.set_ylabel('dI/dE',fontsize=18)\n",
    "            ax.set_yticks([-0.002, -0.001, 0, 0.001])\n",
    "            ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "            ax.legend(loc=2, fontsize=16)\n",
    "\n",
    "    for j in ([17,22,23]):\n",
    "        if i == 1:\n",
    "            ax.axhline(y=1, linestyle='-', color='gray')\n",
    "            p1 = ax.plot(df_dx['x%(i)s'% {\"i\": j}], \\\n",
    "                         np.divide(df_dx['derivative y14'],df_dx['derivative y%(i)s'%{\"i\": j}]), 'k--', label='Ratio sp4/sp'%{\"i\":j})\n",
    "\n",
    "            ax.axvline(x=1.65, linestyle='--')\n",
    "            ax.set_ylim([-1, 2])\n",
    "            ax.set_xlim([.5,3.5])\n",
    "            ax.set_ylabel('R = dI/dE(sample) / dI/dE(vac)', fontsize=18)\n",
    "            ax.set_xlabel('$\\Delta$E (eV)', fontsize=218)\n",
    "            ax.legend()\n",
    "\n",
    "    if i == 0:\n",
    "        ax.tick_params(labelbottom=True)\n",
    "        ax.tick_params(which='major', length= 10, labelsize=18)\n",
    "        ax.tick_params(which='minor', length= 10, labelsize=10)\n",
    "    if i == 1:\n",
    "        ax.set_xlabel('Energy loss (eV)', fontsize=24)\n",
    "        ax.tick_params(length= 10, labelsize=18)\n",
    "        ax.tick_params(which='major', length= 10, labelsize=18)\n",
    "        ax.tick_params(which='minor', length= 10, labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Derivatives.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Where to add pseudo data? ($\\Delta E_{II} = \\Delta E_{pd,min}$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vacmean = pd.DataFrame()\n",
    "nbins = 150\n",
    "df_vacuum = load_data.df_vacuum[(load_data.df_vacuum['x_shifted'] < 20) & (load_data.df_vacuum['x_shifted'] > -.5)]\n",
    "df_vacmean['x'] = np.linspace(df_vacuum['x_shifted'].min(),df_vacuum['x_shifted'].max(), nbins)\n",
    "df_vacmean['y'], df_vacmean['sigma'] = functions.binned_statistics(df_vacuum['x_shifted'], (df_vacuum['y']), nbins)[0:2]\n",
    "df_vacmean['ratio'] = np.divide(df_vacmean['y'], df_vacmean['sigma'])\n",
    "\n",
    "dE2 = df_vacmean['x'][df_vacmean['ratio'] < 1].min()\n",
    "dE2 = np.round(dE2)\n",
    "print(\"The value for dE_II is\", (dE2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 1,1\n",
    "gs = matplotlib.gridspec.GridSpec(nrows,ncols)\n",
    "plt.figure(figsize=(ncols*5,nrows*3.5))\n",
    "\n",
    "cm_subsection = np.linspace(0,1,24)\n",
    "colors = [cm.viridis(x) for x in cm_subsection]\n",
    "\n",
    "hfont = rc('font',**{'family':'sans-serif','sans-serif':['Sans Serif']})\n",
    "\n",
    "ax = plt.subplot(gs[0])\n",
    "ax.set_title('Intensity to sigma ratio', fontsize=16)\n",
    "ax.set_xlim([-1,15])\n",
    "ax.set_xlabel('Energy loss (eV)', fontsize=14)\n",
    "ax.tick_params(which='major',direction='in',length=7, labelsize=14)\n",
    "ax.tick_params(which='minor',length=8)\n",
    "p1 = ax.plot(df_vacmean['x'],functions.smooth(np.divide(df_vacmean['y'], df_vacmean['sigma']), 10), color=colors[0])\n",
    "ax.axhline(y=1, linestyle='-')\n",
    "ax.axvline(x=dE2, linestyle='dotted', linewidth='2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The values of dE1 and dE2:', np.round(dE1,2), \"eV and\", dE2, \"eV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print('The values of dE1 and dE2:', np.round(dE1,2), \"eV and\", dE2, \"eV\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Window and prepare data\n",
    "* Drop all data for $\\Delta$E $> \\Delta$E$_1$\n",
    "* Calculate the binned mean and variance of the spectra together with the function 'binned_statistics(x,y,nbins)'\n",
    "* Returns two pd.DataFrames 'df_mean' and 'df_vacmean' with the mean and variance data of the in-sample and in-vacuum spectra\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "df_window = load_data.df[(load_data.df['x_shifted'] < dE1) & (load_data.df['x_shifted'] > -.5)]\n",
    "df_window_vacuum = load_data.df_vacuum[(load_data.df_vacuum['x_shifted'] <= dE1) & (load_data.df_vacuum['x_shifted'] > -.5)]\n",
    "\n",
    "df_mean, df_vacmean = pd.DataFrame(), pd.DataFrame()\n",
    "nbins = 30\n",
    "\n",
    "df_mean['x'] = np.linspace(df_window['x_shifted'].min(),df_window['x_shifted'].max(), nbins)\n",
    "df_mean['y'], df_mean['sigma'] = functions.binned_statistics(df_window['x_shifted'], np.log(df_window['y']), nbins)[0:2]\n",
    "\n",
    "df_vacmean['x'] = np.linspace(df_window_vacuum['x_shifted'].min(),df_window_vacuum['x_shifted'].max(), nbins)\n",
    "df_vacmean['y'], df_vacmean['sigma'] = functions.binned_statistics(df_window_vacuum['x_shifted'], np.log(df_window_vacuum['y']), nbins)[0:2]\n",
    "\n",
    "print(\"Training data points for DeltaE > DeltaE_I have been removed.\")\n",
    "print(\"Experimental mean and sigma are calculated.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pseudo data for $\\Delta$E $ > \\Delta$E$_{II}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = dE2\n",
    "max_x = 16\n",
    "N_pseudo = 20\n",
    "\n",
    "df_pseudo = pd.DataFrame({'x':np.linspace(min_x, max_x, N_pseudo),'y': .5 * np.ones(N_pseudo), \\\n",
    "                    'sigma': .08 * np.ones(N_pseudo)})\n",
    "df_full = pd.concat([df_mean, df_pseudo])\n",
    "\n",
    "print('Pseudo data points added for Delta E > DeltaE_II')\n",
    "print('Training data set \"df_full\" has been created')\n",
    "\n",
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize the NN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(inputs, n_outputs):\n",
    "    hidden_layer_1 = tf.layers.dense(inputs, 10, activation=tf.nn.sigmoid)\n",
    "    hidden_layer_2 = tf.layers.dense(hidden_layer_1, 15, activation=tf.nn.sigmoid)\n",
    "    hidden_layer_3 = tf.layers.dense(hidden_layer_2, 5, activation=tf.nn.relu)\n",
    "    output = tf.layers.dense(hidden_layer_3, n_outputs, name='outputs', reuse=tf.AUTO_REUSE)\n",
    "    return output\n",
    "\n",
    "print(\"NN is initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Initialize data for NN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function: $\\chi^2 = 1/N \\sum (y - y_{pred})^2 / \\sigma^2$ \\\n",
    "Optimizer: AdamOptimizer\n",
    "\n",
    "Training points are generated by Monte Carlo sampling; for the set of training points $[\\Delta E_i, D_i, \\sigma_i]$, a set of MC training points is generated by adding a stochastic noise signal on top of the the data with a std equal to the corresponding error on that point.\n",
    "\n",
    "- **train_x**: $\\Delta E_i$\n",
    "- **train_y**: $D_i$ + rand.norm(0, $\\sigma_i$)\n",
    "\n",
    "Repetitive training of the NN (number of repetitions = $N_{rep}$) on each set of MC pseudo data yields a prediction that is distributed with a mean and std corresponding to the mean and error of the original training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_default_graph\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 1], name=\"x\")\n",
    "y = tf.placeholder(\"float\", [None, 1], name=\"y\")\n",
    "sigma = tf.placeholder(\"float\", [None, 1], name=\"sigma\")\n",
    "\n",
    "predictions = make_model(x,1)\n",
    "\n",
    "df_train_full = df_full\n",
    "df_train_full = df_train_full.drop_duplicates(subset = ['x']) # Only keep one copy per x-value\n",
    "\n",
    "N_full = len(df_train_full['x'])\n",
    "\n",
    "full_x = np.copy(df_train_full['x']).reshape(N_full,1)\n",
    "full_y = np.copy(df_train_full['y']).reshape(N_full,1)\n",
    "full_sigma = np.copy(df_train_full['sigma']).reshape(N_full,1)\n",
    "\n",
    "N_pred = 3000\n",
    "pred_min = -.5\n",
    "pred_max = 20\n",
    "predict_x = np.linspace(pred_min,pred_max,N_pred).reshape(N_pred,1)\n",
    "\n",
    "print(\"Dataset is split into train subset (80%) and validation subset (20%)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(full_x, full_y, '.', label='train')\n",
    "\n",
    "plt.axvline(x=dE1, color='lightgray')\n",
    "plt.axvline(x=dE2, color='lightgray')\n",
    "plt.title('Visualization of training data', fontsize=15)\n",
    "plt.ylabel('Log intensity', fontsize=14)\n",
    "plt.xlabel('Energy loss (eV)', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Create MC replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "Nrep = 500\n",
    "\n",
    "full_y_reps = np.zeros(shape=(N_full, Nrep))\n",
    "i=0\n",
    "while i < Nrep:\n",
    "        full_rep = np.random.normal(0, full_sigma)\n",
    "        full_y_reps[:,i] = (full_y + full_rep).reshape(N_full)\n",
    "        i+=1\n",
    "\n",
    "std_reps = np.std(full_y_reps, axis=1)\n",
    "mean_reps = np.mean(full_y_reps, axis=1)\n",
    "\n",
    "print('MC pseudo data has been created for %(nrep)s replicas' %{\"nrep\": Nrep})\n",
    "\n",
    "\n",
    "N_train = int(.8 * N_full)\n",
    "N_test = int(.2 * N_full)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "def function_train():\n",
    "\n",
    "    chi_array = []\n",
    "\n",
    "    cost = tf.reduce_mean(tf.square((y-predictions)/sigma), name=\"cost_function\")\n",
    "    eta = 6e-3\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=eta, decay=0.9, momentum=0.0, epsilon=1e-10).minimize(cost)\n",
    "    saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "    print(\"Start training on\", '%04d'%(N_train), \"and validating on\",'%0.4d'%(N_test), \"samples\")\n",
    "\n",
    "    for i in range(Nrep):\n",
    "\n",
    "        full_y = full_y_reps[:, i].reshape(N_full,1)\n",
    "\n",
    "        train_x, test_x, train_y, test_y, train_sigma, test_sigma = \\\n",
    "            train_test_split(full_x, full_y, full_sigma, test_size=.2)\n",
    "\n",
    "        print(len(train_x))\n",
    "        train_x, test_x = train_x.reshape(N_train,1), test_x.reshape(N_test,1)\n",
    "        train_y, test_y = train_y.reshape(N_train,1), test_y.reshape(N_test,1)\n",
    "        train_sigma, test_sigma = train_sigma.reshape(N_train,1), test_sigma.reshape(N_test,1)\n",
    "\n",
    "\n",
    "        ### Train and validate\n",
    "        prev_test_cost = 0\n",
    "        prev_epoch = 0\n",
    "        avg_cost = 0\n",
    "\n",
    "        array_train = []\n",
    "        array_test = []\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            training_epochs = 25000\n",
    "            display_step = 1000\n",
    "\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                _, c = sess.run([optimizer, cost],\n",
    "                                feed_dict={\n",
    "                                    x: train_x,\n",
    "                                    y: train_y,\n",
    "                                    sigma: train_sigma\n",
    "                                })\n",
    "\n",
    "                avg_cost = c\n",
    "                print(c)\n",
    "                sys.exit(0)\n",
    "\n",
    "                test_cost = cost.eval({x: test_x, y: test_y, sigma: test_sigma})\n",
    "\n",
    "\n",
    "                if epoch % display_step == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"| Training cost=\", \"{:.9f}\".format(avg_cost), \"| Validation cost=\", \"{:.9f}\".format(test_cost))\n",
    "                    array_train.append(avg_cost)\n",
    "                    array_test.append(test_cost)\n",
    "                    saver.save(sess, 'Models/All_models/my-model.ckpt', global_step=epoch , write_meta_graph=False)\n",
    "\n",
    "\n",
    "                elif test_cost < prev_test_cost:\n",
    "                    prev_test_cost = test_cost\n",
    "                    prev_epoch = epoch\n",
    "\n",
    "            best_iteration = np.argmin(array_test)\n",
    "            best_epoch = best_iteration * display_step\n",
    "            best_model = 'Models/All_models/my-model.ckpt-%(s)s' % {'s': best_epoch}\n",
    "\n",
    "            print(\"Optimization %(i)s Finished! Best model after epoch %(s)s\" % {'i': i, 's': best_epoch})\n",
    "\n",
    "            dt_string = now.strftime(\"%d.%m.%Y %H:%M:%S\")\n",
    "            d_string = now.strftime(\"%d.%m.%Y\")\n",
    "            t_string = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "            saver.restore(sess, best_model)\n",
    "            saver.save(sess, 'Models/Best_models/%(s)s/best_model_%(i)s' % {'s': d_string, 'i': i})\n",
    "\n",
    "\n",
    "            predictions_values = sess.run(predictions,\n",
    "                                feed_dict={\n",
    "                                    x: train_x,\n",
    "                                    y: train_y\n",
    "                                })\n",
    "\n",
    "\n",
    "            extrapolation = sess.run(predictions,\n",
    "                                feed_dict={\n",
    "                                    x: predict_x\n",
    "                                })\n",
    "\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "\n",
    "        nownow = datetime.now()\n",
    "        print(\"time elapsed\", nownow-now)\n",
    "\n",
    "        a = np.array(train_x).reshape(N_train,)\n",
    "        b = np.array(train_y).reshape(N_train,)\n",
    "        c = np.array(predictions_values).reshape(N_train,)\n",
    "\n",
    "        d = array_train\n",
    "        e = array_test\n",
    "\n",
    "        k = np.array(predict_x).reshape(N_pred,)\n",
    "        l = np.array(extrapolation).reshape(N_pred,)\n",
    "\n",
    "        path_to_data = 'Data/Results/%(date)s/'% {\"date\": d_string}\n",
    "\n",
    "        np.savetxt(path_to_data + 'Predictions_%(k)s.csv' % {\"k\": i}, list(zip(a,b,c)),  delimiter=',', fmt='%f')\n",
    "        np.savetxt(path_to_data + 'Cost_%(k)s.csv' % {\"k\": i}, list(zip(d,e)),  delimiter=',',fmt='%f')\n",
    "        np.savetxt(path_to_data + 'Extrapolation_%(k)s.csv' % {\"k\":i}, list(zip(k, l)),  delimiter=',', fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Serial training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "function_train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-d110f80d",
   "language": "python",
   "display_name": "PyCharm (tutorial_notebooks)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}